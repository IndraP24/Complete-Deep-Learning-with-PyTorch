{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, val split\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train, val = random_split(train_data, [55000, 5000])\n",
    "train_loader = DataLoader(train, batch_size=64)\n",
    "val_loader = DataLoader(val, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 64),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(64, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a more flexible model\n",
    "class ResNet(nn.Module): # --> Model with residual connections\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = nn.functional.relu(self.l1(x))\n",
    "        h2 = nn.functional.relu(self.l2(h1))\n",
    "        do = self.do(h2 + h1)\n",
    "        logits = self.l3(do)\n",
    "        return logits\n",
    "\n",
    "model = ResNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "params = model.parameters()\n",
    "optimizer = optim.SGD(params, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Training Loss: 1.18 | Training Accuracy: 0.70\n",
      "Validation Loss: 0.54 | Validation Accuracy: 0.87\n",
      "\n",
      "Epoch 2:\n",
      "Training Loss: 0.49 | Training Accuracy: 0.86\n",
      "Validation Loss: 0.38 | Validation Accuracy: 0.89\n",
      "\n",
      "Epoch 3:\n",
      "Training Loss: 0.39 | Training Accuracy: 0.89\n",
      "Validation Loss: 0.33 | Validation Accuracy: 0.90\n",
      "\n",
      "Epoch 4:\n",
      "Training Loss: 0.34 | Training Accuracy: 0.90\n",
      "Validation Loss: 0.30 | Validation Accuracy: 0.91\n",
      "\n",
      "Epoch 5:\n",
      "Training Loss: 0.31 | Training Accuracy: 0.91\n",
      "Validation Loss: 0.28 | Validation Accuracy: 0.92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    losses = list()\n",
    "    accuracies = list()\n",
    "    model.train() # --> Since Dropouts are used\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b * 1 * 28 * 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1).cuda()\n",
    "\n",
    "        # 1: Forward Prop\n",
    "        logits = model(x)\n",
    "\n",
    "        # 2: Compute Objective / Loss Function\n",
    "        J = loss(logits, y.cuda())\n",
    "\n",
    "        # 3: Cleaning / Updating the Gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 4: Accumulate the Partial Derivatives of J wrt params\n",
    "        J.backward()\n",
    "\n",
    "        # 5: Step on the opposite direction of the gradient\n",
    "        optimizer.step()\n",
    "        # with torch.no_grad(): params = params - eta * params.grad\n",
    "        losses.append(J.item())\n",
    "        accuracies.append(y.eq(logits.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f'Epoch {epoch+1}', end=':\\n') \n",
    "    print(f'Training Loss: {torch.tensor(losses).mean():.2f}', end=' | ')\n",
    "    print(f'Training Accuracy: {torch.tensor(accuracies).mean():.2f}')\n",
    "\n",
    "\n",
    "    losses = list()\n",
    "    accuracies = list()\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        # x: b * 1 * 28 * 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1).cuda()\n",
    "\n",
    "        # 1: Forward Prop\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "\n",
    "        # 2: Compute Objective / Loss Function\n",
    "        J = loss(logits, y.cuda())\n",
    "\n",
    "        losses.append(J.item())\n",
    "        accuracies.append(y.eq(logits.detach().argmax(dim=1).cpu()).float().mean())\n",
    "\n",
    "    print(f'Validation Loss: {torch.tensor(losses).mean():.2f}', end=' | ')\n",
    "    print(f'Validation Accuracy: {torch.tensor(accuracies).mean():.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bc8a72d961133e51c159ac9d943613f233897aed07edd92e91710696a38f937"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('ptDL-39': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
